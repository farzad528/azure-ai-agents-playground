{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using connection ID: /subscriptions/ee787b9b-a25f-4d20-86e9-45fcea5253dd/resourceGroups/fsunavala-sandbox/providers/Microsoft.MachineLearningServices/workspaces/fsunavala-aml-sandbox/connections/AzureAISearch\n",
      "Created agent with ID: asst_C5ZWaDxrTx1Frb2Dpi5GHbAt\n",
      "Created thread with ID: thread_06Eu6mTlO7ucAVQ5pMjlz6qq\n",
      "User message created.\n",
      "Run completed with status: RunStatus.COMPLETED\n",
      "Assistant reply: Binary quantization in Azure Search is a method used for compressing embeddings, particularly when dealing with high-dimensional data (greater than 1024 dimensions). Binary quantization is effective when the embeddings are centered around zero, which is a common trait for popular embedding models like OpenAI's and Cohere's.\n",
      "\n",
      "Key points about binary quantization in Azure Search include:\n",
      "\n",
      "1. **Efficiency**: It significantly reduces the storage space required for vector storage and improves query response times.\n",
      "2. **Combination with MRL**: When used alongside multilevel compression (MRL), binary quantization provides maximum vector index size reduction. MRL saving features work best when combined with binary quantization, allowing for even greater storage reductions.\n",
      "3. **Configuration**: To utilize this feature, you need to specify a vector search configuration in your index definition, choosing either scalar or binary quantization (binary is recommended).\n",
      "\n",
      "This feature is currently in preview as of 2024-09-01 and requires specific configurations in the index definition【7:0†source】.\n",
      "Tracing complete. Check the 'Tracing' tab in your Azure AI Foundry project page.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from opentelemetry import trace\n",
    "from azure.monitor.opentelemetry import configure_azure_monitor\n",
    "\n",
    "# --- Telemetry Configuration ---\n",
    "connection_string = os.environ[\"AZURE_CONNECTION_STRING\"]\n",
    "\n",
    "# Create the AIProjectClient using DefaultAzureCredential.\n",
    "project_client = AIProjectClient.from_connection_string(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    conn_str=connection_string,\n",
    ")\n",
    "\n",
    "# Get the Application Insights connection string from your project.\n",
    "app_insights_conn_str = project_client.telemetry.get_connection_string()\n",
    "\n",
    "# Configure Azure Monitor tracing.\n",
    "configure_azure_monitor(connection_string=app_insights_conn_str)\n",
    "\n",
    "# (Optional) Enable verbose logging of telemetry to stdout for local debugging.\n",
    "project_client.telemetry.enable(destination=sys.stdout)\n",
    "\n",
    "# Set up a tracer.\n",
    "tracer = trace.get_tracer(__name__)\n",
    "\n",
    "# --- Main Operations under a Tracing Span ---\n",
    "with tracer.start_as_current_span(\"simple-agent-sample\"):\n",
    "    # Use environment variable directly for the search connection name.\n",
    "    SEARCH_CONNECTION_NAME = os.environ[\"AI_SEARCH_CONNECTION_NAME\"]\n",
    "    connections = list(project_client.connections.list())\n",
    "    conn_id = next(c.id for c in connections if c.name == SEARCH_CONNECTION_NAME)\n",
    "    print(\"Using connection ID:\", conn_id)\n",
    "\n",
    "    # Create the Azure AI Search tool.\n",
    "    SEARCH_INDEX_NAME = os.environ.get(\"AI_SEARCH_INDEX_NAME\", \"azure-search-docs\")\n",
    "    from azure.ai.projects.models import AzureAISearchTool\n",
    "    search_tool = AzureAISearchTool(\n",
    "        index_connection_id=conn_id,\n",
    "        index_name=SEARCH_INDEX_NAME,\n",
    "    )\n",
    "\n",
    "    # Create an agent.\n",
    "    agent = project_client.agents.create_agent(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        name=\"my-assistant\",\n",
    "        instructions=\"You are a helpful assistant.\",\n",
    "        tools=search_tool.definitions,\n",
    "        tool_resources=search_tool.resources,\n",
    "    )\n",
    "    print(\"Created agent with ID:\", agent.id)\n",
    "\n",
    "    # Create a conversation thread.\n",
    "    thread = project_client.agents.create_thread()\n",
    "    print(\"Created thread with ID:\", thread.id)\n",
    "\n",
    "    # Send a user message.\n",
    "    project_client.agents.create_message(\n",
    "        thread_id=thread.id,\n",
    "        role=\"user\",\n",
    "        content=\"From my index, what is binary quantization in Azure Search?\",\n",
    "    )\n",
    "    print(\"User message created.\")\n",
    "\n",
    "    # Process the run synchronously (poll until completion).\n",
    "    run = project_client.agents.create_and_process_run(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=agent.id,\n",
    "    )\n",
    "    while run.status in [\"queued\", \"in_progress\", \"requires_action\"]:\n",
    "        time.sleep(1)\n",
    "        run = project_client.agents.get_run(thread_id=thread.id, run_id=run.id)\n",
    "        print(\"Current run status:\", run.status)\n",
    "\n",
    "    print(\"Run completed with status:\", run.status)\n",
    "\n",
    "    # Retrieve and print the assistant's reply.\n",
    "    messages = project_client.agents.list_messages(thread_id=thread.id)\n",
    "    assistant_replies = [m for m in messages.data if m.role == \"assistant\"]\n",
    "    if assistant_replies:\n",
    "        reply = assistant_replies[-1].content[0].text.value\n",
    "        print(\"Assistant reply:\", reply)\n",
    "    else:\n",
    "        print(\"No assistant reply received.\")\n",
    "\n",
    "print(\"Tracing complete. Check the 'Tracing' tab in your Azure AI Foundry project page.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
