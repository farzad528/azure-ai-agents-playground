{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mem0 Azure AI Fondry Integration - Quickstart\n",
    "\n",
    "This guide demonstrates how to use Mem0 with Azure AI Search and Azure OpenAI for adding persistent memory to your AI applications. We'll cover:\n",
    "\n",
    "- Basic memory operations with Mem0 and Azure AI Search\n",
    "- A practical travel planning assistant example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install mem0ai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "First, set your Azure environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mem0 import Memory\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Load Azure OpenAI configuration\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_CHAT_COMPLETION_DEPLOYED_MODEL_NAME = os.getenv(\"AZURE_OPENAI_CHAT_COMPLETION_DEPLOYED_MODEL_NAME\")\n",
    "AZURE_OPENAI_EMBEDDING_DEPLOYED_MODEL_NAME = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYED_MODEL_NAME\")\n",
    "\n",
    "# Load Azure AI Search configuration\n",
    "SEARCH_SERVICE_ENDPOINT = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")\n",
    "SEARCH_SERVICE_API_KEY = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "SEARCH_SERVICE_NAME=\"fsunavala-ai-search\"\n",
    "\n",
    "# Create Azure OpenAI client\n",
    "azure_openai_client = AzureOpenAI(\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=\"2024-10-21\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Memory Operations\n",
    "Here's a minimal working example of storing and retrieving memories with Mem0 and Azure AI Search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem0 initialized with Azure AI Search\n",
      "\n",
      "===== STORING MEMORIES =====\n",
      "✓ Added memory about hiking preferences\n",
      "✓ Added conversation about Japan trip\n",
      "✓ Added travel preferences with metadata\n",
      "\n",
      "===== SEARCHING MEMORIES =====\n",
      "Found 3 relevant memories:\n",
      "1. Planning a trip to Japan in the fall (Score: 0.6160)\n",
      "2. Would like to visit Tokyo and Kyoto (Score: 0.6017)\n",
      "3. Usually brings own headphones on flights (Score: 0.5921)\n",
      "\n",
      "===== ALL MEMORIES =====\n",
      "Total memories: 7\n",
      "1. Maybe see Mount Fuji\n",
      "2. Usually brings own headphones on flights\n",
      "3. Enjoy taking landscape photos\n",
      "4. Planning a trip to Japan in the fall\n",
      "5. Enjoy hiking in national parks\n",
      "6. Would like to visit Tokyo and Kyoto\n",
      "7. Prefers window seats on long flights\n",
      "\n",
      "Basic operations demonstrated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Basic example for Azure AI Search with Mem0\n",
    "import os\n",
    "from mem0 import Memory\n",
    "\n",
    "# Load Azure configuration\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_CHAT_COMPLETION_DEPLOYED_MODEL_NAME= os.getenv(\"AZURE_OPENAI_CHAT_COMPLETION_DEPLOYED_MODEL_NAME\")\n",
    "AZURE_OPENAI_EMBEDDING_DEPLOYED_MODEL_NAME = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYED_MODEL_NAME\")\n",
    "SEARCH_SERVICE_ENDPOINT = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")\n",
    "SEARCH_SERVICE_API_KEY = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "SEARCH_SERVICE_NAME = \"fsunavala-ai-search\"\n",
    "\n",
    "# Configure Mem0 with Azure AI Search\n",
    "memory_config = {\n",
    "    \"vector_store\": {\n",
    "        \"provider\": \"azure_ai_search\",\n",
    "        \"config\": {\n",
    "            \"service_name\": SEARCH_SERVICE_NAME,\n",
    "            \"api_key\": SEARCH_SERVICE_API_KEY,\n",
    "            \"collection_name\": \"memories\",\n",
    "            \"embedding_model_dims\": 1536,\n",
    "        },\n",
    "    },\n",
    "    \"embedder\": {\n",
    "        \"provider\": \"azure_openai\",\n",
    "        \"config\": {\n",
    "            \"model\": AZURE_OPENAI_EMBEDDING_DEPLOYED_MODEL_NAME,\n",
    "            \"embedding_dims\": 1536,\n",
    "            \"azure_kwargs\": {\n",
    "                \"api_version\": \"2024-10-21\",\n",
    "                \"azure_deployment\": AZURE_OPENAI_EMBEDDING_DEPLOYED_MODEL_NAME,\n",
    "                \"azure_endpoint\": AZURE_OPENAI_ENDPOINT,\n",
    "                \"api_key\": AZURE_OPENAI_API_KEY,\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    \"llm\": {\n",
    "        \"provider\": \"azure_openai\",\n",
    "        \"config\": {\n",
    "            \"model\": AZURE_OPENAI_CHAT_COMPLETION_DEPLOYED_MODEL_NAME,\n",
    "            \"temperature\": 0.1,\n",
    "            \"max_tokens\": 2000,\n",
    "            \"azure_kwargs\": {\n",
    "                \"azure_deployment\": AZURE_OPENAI_CHAT_COMPLETION_DEPLOYED_MODEL_NAME, \n",
    "                \"api_version\": \"2024-10-21\",\n",
    "                \"azure_endpoint\": AZURE_OPENAI_ENDPOINT,\n",
    "                \"api_key\": AZURE_OPENAI_API_KEY,\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    \"version\": \"v1.1\",\n",
    "}\n",
    "# Initialize memory\n",
    "memory = Memory.from_config(memory_config)\n",
    "print(\"Mem0 initialized with Azure AI Search\")\n",
    "\n",
    "# Basic memory operations\n",
    "def demonstrate_basic_operations():\n",
    "    user_id = \"demo_user\"\n",
    "    \n",
    "    # 1. Add a simple memory\n",
    "    print(\"\\n===== STORING MEMORIES =====\")\n",
    "    memory.add(\n",
    "        \"I enjoy hiking in national parks and taking landscape photos.\",\n",
    "        user_id=user_id\n",
    "    )\n",
    "    print(\"✓ Added memory about hiking preferences\")\n",
    "    \n",
    "    # 2. Add conversation messages\n",
    "    conversation = [\n",
    "        {\"role\": \"user\", \"content\": \"I'm planning a trip to Japan in the fall.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"That's a great time to visit Japan! The autumn colors are beautiful.\"},\n",
    "        {\"role\": \"user\", \"content\": \"I'd like to visit Tokyo and Kyoto, and maybe see Mount Fuji.\"}\n",
    "    ]\n",
    "    memory.add(conversation, user_id=user_id)\n",
    "    print(\"✓ Added conversation about Japan trip\")\n",
    "    \n",
    "    # 3. Add memory with metadata\n",
    "    memory.add(\n",
    "        \"I prefer window seats on long flights and usually bring my own headphones.\",\n",
    "        user_id=user_id,\n",
    "        metadata={\"category\": \"travel_preferences\", \"importance\": \"medium\"}\n",
    "    )\n",
    "    print(\"✓ Added travel preferences with metadata\")\n",
    "    \n",
    "    # 4. Search memories\n",
    "    print(\"\\n===== SEARCHING MEMORIES =====\")\n",
    "    search_results = memory.search(\"What are this user's travel plans?\", user_id=user_id, limit=3)\n",
    "    print(f\"Found {len(search_results['results'])} relevant memories:\")\n",
    "    for i, result in enumerate(search_results['results'], 1):\n",
    "        print(f\"{i}. {result['memory']} (Score: {result['score']:.4f})\")\n",
    "    \n",
    "    # 5. Get all memories\n",
    "    print(\"\\n===== ALL MEMORIES =====\")\n",
    "    all_memories = memory.get_all(user_id=user_id)\n",
    "    print(f\"Total memories: {len(all_memories['results'])}\")\n",
    "    for i, memory_item in enumerate(all_memories['results'], 1):\n",
    "        print(f\"{i}. {memory_item['memory']}\")\n",
    "    \n",
    "    return \"Basic operations demonstrated successfully!\"\n",
    "\n",
    "# Run the demonstration\n",
    "result = demonstrate_basic_operations()\n",
    "print(f\"\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Travel Planning Assistant with Mem0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== TRAVEL PLANNING ASSISTANT DEMO ===\n",
      "\n",
      "Travel Assistant initialized for user farzad_london_2025\n",
      "User: Hi, my name is Farzad. I'm planning a business trip to London next month for about 5 days.\n",
      "Assistant: Hi Farzad! That sounds great. For your business trip to London, here are some tips to consider:\n",
      "\n",
      "1. **Accommodation**: Look for hotels in the City of London or near Canary Wharf for easy access to business hubs. The Montcalm Royal London House or the Hilton London Bankside are good options.\n",
      "\n",
      "2. **Transportation**: Get an Oyster card or a contactless payment card for convenient travel on the Tube and buses. The London Underground is efficient for getting around the city.\n",
      "\n",
      "3. **Meeting Venues**: If you need to book meeting rooms, consider venues like the British Library Business & IP Centre or the Barbican Centre, which offer great facilities.\n",
      "\n",
      "4. **Dining**: For business dinners, try Hawksmoor for steak or Dishoom for a lively Indian dining experience.\n",
      "\n",
      "5. **Networking**: Look for networking events or conferences happening during your stay on platforms like Meetup or Eventbrite.\n",
      "\n",
      "Let me know if you need specific recommendations or assistance with anything else!\n",
      "\n",
      "User: I need recommendations for fish and chips restaurants near London Bridge cause I love the taste!\n",
      "Including 2 memories in context\n",
      "Assistant: Hi Farzad! Here are some great fish and chips restaurants near London Bridge that you might enjoy:\n",
      "\n",
      "1. **The Fish & Chip Shop** - Located on the nearby Borough High Street, this spot is known for its classic offerings and quality ingredients.\n",
      "\n",
      "2. **Poppies Fish & Chips** - Situated a short walk away in Spitalfields, it’s famous for its authentic, retro atmosphere and delicious fish and chips.\n",
      "\n",
      "3. **Fish! Kitchen** - Close to Borough Market, this restaurant serves sustainably sourced fish and offers a modern twist on the traditional dish.\n",
      "\n",
      "4. **The Anchor** - Located along the River Thames, this pub serves excellent fish and chips with a lovely view of the river.\n",
      "\n",
      "Enjoy your trip, and let me know if you need more recommendations!\n",
      "\n",
      "\n",
      "===== STORED MEMORIES (3) =====\n",
      "1. Loves the taste of fish and chips\n",
      "2. Planning a business trip to London next month for about 5 days\n",
      "3. Name is Farzad\n",
      "==============================\n",
      "\n",
      "Search Query: 'What are Farzad's preferences for food in London?'\n",
      "Found 4 relevant memories:\n",
      "1. Name is Farzad (Score: 0.6696)\n",
      "2. Needs recommendations for fish and chips restaurants near London Bridge (Score: 0.6564)\n",
      "3. Loves the taste of fish and chips (Score: 0.6324)\n"
     ]
    }
   ],
   "source": [
    "# Part 2: Travel Planning Assistant with Memory\n",
    "\n",
    "class TravelAssistant:\n",
    "    def __init__(self, user_id):\n",
    "        \"\"\"Initialize travel assistant with memory for a specific user\"\"\"\n",
    "        self.user_id = user_id\n",
    "        \n",
    "        # Configure memory for travel planning\n",
    "        memory_config = {\n",
    "            \"vector_store\": {\n",
    "                \"provider\": \"azure_ai_search\",\n",
    "                \"config\": {\n",
    "                    \"service_name\": SEARCH_SERVICE_NAME,\n",
    "                    \"api_key\": SEARCH_SERVICE_API_KEY,\n",
    "                    \"collection_name\": \"travel_memories\",\n",
    "                    \"embedding_model_dims\": 1536,\n",
    "                    \"compression_type\": \"binary\",\n",
    "                },\n",
    "            },\n",
    "            \"llm\": {\n",
    "                \"provider\": \"azure_openai\",\n",
    "                \"config\": {\n",
    "                    \"model\": AZURE_OPENAI_CHAT_COMPLETION_DEPLOYED_MODEL_NAME,\n",
    "                    \"temperature\": 0.7,\n",
    "                    \"max_tokens\": 800,\n",
    "                    \"azure_kwargs\": {\n",
    "                        \"azure_deployment\": AZURE_OPENAI_CHAT_COMPLETION_DEPLOYED_MODEL_NAME,\n",
    "                        \"api_version\": \"2024-10-21\",\n",
    "                        \"azure_endpoint\": AZURE_OPENAI_ENDPOINT,\n",
    "                        \"api_key\": AZURE_OPENAI_API_KEY,\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "            \"embedder\": {\n",
    "                \"provider\": \"azure_openai\",\n",
    "                \"config\": {\n",
    "                    \"model\": AZURE_OPENAI_EMBEDDING_DEPLOYED_MODEL_NAME,\n",
    "                    \"embedding_dims\": 1536,\n",
    "                    \"azure_kwargs\": {\n",
    "                        \"api_version\": \"2024-10-21\",\n",
    "                        \"azure_deployment\": AZURE_OPENAI_EMBEDDING_DEPLOYED_MODEL_NAME,\n",
    "                        \"azure_endpoint\": AZURE_OPENAI_ENDPOINT,\n",
    "                        \"api_key\": AZURE_OPENAI_API_KEY,\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "            \"version\": \"v1.1\",\n",
    "        }\n",
    "        \n",
    "        self.memory = Memory.from_config(memory_config)\n",
    "        self.azure_client = azure_openai_client\n",
    "        print(f\"Travel Assistant initialized for user {user_id}\")\n",
    "    \n",
    "    def get_response(self, query, memory_context=True):\n",
    "        \"\"\"Get response from Azure OpenAI with memory context\"\"\"\n",
    "        # Retrieve relevant memories if enabled\n",
    "        memory_text = \"\"\n",
    "        if memory_context:\n",
    "            memories = self.memory.search(query, user_id=self.user_id)\n",
    "            if 'results' in memories and memories['results']:\n",
    "                memory_text = \"\\n\\nRelevant information from previous conversations:\\n\"\n",
    "                for i, mem in enumerate(memories['results'][:5], 1):\n",
    "                    memory_text += f\"{i}. {mem['memory']}\\n\"\n",
    "                print(f\"Including {len(memories['results'][:5])} memories in context\")\n",
    "            else:\n",
    "                print(\"No relevant memories found\")\n",
    "        \n",
    "        # Construct messages with system prompt and memory context\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": \"You are a helpful travel assistant for London travel planning. \"\n",
    "                           \"Be concise, specific, and helpful. Refer to the user by name when appropriate. \"\n",
    "                           \"Recommend specific places when relevant.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": f\"{query}\\n{memory_text}\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Get response from Azure OpenAI\n",
    "        response = self.azure_client.chat.completions.create(\n",
    "            model=AZURE_OPENAI_CHAT_COMPLETION_DEPLOYED_MODEL_NAME,\n",
    "            messages=messages,\n",
    "            temperature=0.7,\n",
    "            max_tokens=800,\n",
    "        )\n",
    "        \n",
    "        # Extract response content\n",
    "        response_content = response.choices[0].message.content\n",
    "        \n",
    "        # Store the conversation in memory\n",
    "        conversation = [\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "            {\"role\": \"assistant\", \"content\": response_content}\n",
    "        ]\n",
    "        self.memory.add(conversation, user_id=self.user_id)\n",
    "        \n",
    "        return response_content\n",
    "    \n",
    "    def verify_memories(self):\n",
    "        \"\"\"Verify what memories have been stored\"\"\"\n",
    "        all_memories = self.memory.get_all(user_id=self.user_id)\n",
    "        print(f\"\\n===== STORED MEMORIES ({len(all_memories['results'])}) =====\")\n",
    "        for i, memory in enumerate(all_memories['results'], 1):\n",
    "            print(f\"{i}. {memory['memory']}\")\n",
    "        print(\"==============================\\n\")\n",
    "        return all_memories\n",
    "\n",
    "# Create travel assistant for Farzad\n",
    "print(\"\\n\\n=== TRAVEL PLANNING ASSISTANT DEMO ===\\n\")\n",
    "assistant = TravelAssistant(user_id=\"farzad_london_2025\")\n",
    "\n",
    "# First interaction - Initial inquiry\n",
    "query1 = \"Hi, my name is Farzad. I'm planning a business trip to London next month for about 5 days.\"\n",
    "print(f\"User: {query1}\")\n",
    "response1 = assistant.get_response(query1, memory_context=False)  # No memories yet\n",
    "print(f\"Assistant: {response1}\\n\")\n",
    "\n",
    "# Second interaction - Specific question about fish and chips\n",
    "query2 = \"I need recommendations for fish and chips restaurants near London Bridge cause I love the taste!\"\n",
    "print(f\"User: {query2}\")\n",
    "response2 = assistant.get_response(query2)  # Should use memory context\n",
    "print(f\"Assistant: {response2}\\n\")\n",
    "\n",
    "# Verify what memories have been stored\n",
    "assistant.verify_memories()\n",
    "\n",
    "# Search for Farzad's food preferences\n",
    "search_query = \"What are Farzad's preferences for food in London?\"\n",
    "print(f\"Search Query: '{search_query}'\")\n",
    "search_results = assistant.memory.search(search_query, user_id=\"farzad_london_2025\")\n",
    "print(f\"Found {len(search_results['results'])} relevant memories:\")\n",
    "for i, result in enumerate(search_results['results'][:3], 1):\n",
    "    print(f\"{i}. {result['memory']} (Score: {result['score']:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
